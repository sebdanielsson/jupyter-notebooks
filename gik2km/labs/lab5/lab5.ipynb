{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GIK2KM - Lab 4 - Text Mining\n",
    "\n",
    "Run the code below to download language models etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Try to load the module\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"Module 'en_core_web_sm' is loaded.\")\n",
    "except OSError:\n",
    "    print(\"Module 'en_core_web_sm' is not installed. Installing now...\")\n",
    "    os.system(\"python -m spacy download en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"If the module still couldn't be loaded, but the installation was succesful, try to restart the kernel.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task1: Sentiment Analysis using polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read data to dataframe\n",
    "df_t1 = pd.read_csv('amazon_alexa.tsv', sep='\\t')\n",
    "\n",
    "# Convert reviews to strings\n",
    "df_t1['reviews'] = df_t1['verified_reviews'].astype(str)\n",
    "\n",
    "df_t1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "\n",
    "# Remove contractions function\n",
    "def remove_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "# Remove contractions\n",
    "df_t1['reviews'] = df_t1['reviews'].apply(remove_contractions)\n",
    "\n",
    "df_t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Remove special characters function\n",
    "def remove_special_characters(text):\n",
    "    return re.sub(r'[^A-Za-z0-9\\s]+', '', text)\n",
    "\n",
    "# Remove special characters\n",
    "df_t1['reviews'] = df_t1['reviews'].apply(remove_special_characters)\n",
    "\n",
    "df_t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokenize function\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# Tokenize\n",
    "df_t1['reviews_tokenized'] = df_t1['reviews'].apply(tokenize)\n",
    "\n",
    "df_t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase function\n",
    "def lowercase(tokens):\n",
    "    return [word.lower() for word in tokens]\n",
    "\n",
    "# Lowercase\n",
    "df_t1['reviews_tokenized'] = df_t1['reviews_tokenized'].apply(lowercase)\n",
    "\n",
    "df_t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Remove stopwords function\n",
    "def remove_stopwords(tokens):\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "# Remove stopwords\n",
    "df_t1['reviews_tokenized'] = df_t1['reviews_tokenized'].apply(remove_stopwords)\n",
    "\n",
    "df_t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Create a WordNet lemmatizer object\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize function\n",
    "def lemmatize(tokens):\n",
    "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "# Lemmatize\n",
    "df_t1['reviews_tokenized'] = df_t1['reviews_tokenized'].apply(lemmatize)\n",
    "\n",
    "df_t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Join the tokens back into strings\n",
    "df_t1['reviews_cleaned'] = df_t1['reviews_tokenized'].apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "# Create the CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the data\n",
    "X = vectorizer.fit_transform(df_t1['reviews_cleaned'])\n",
    "\n",
    "# Count the sum of each word\n",
    "sum_words = X.sum(axis=0)\n",
    "\n",
    "# Create a (word, frequency) list and sort it in descending order\n",
    "words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "\n",
    "# Get the 20 most common words\n",
    "common_words = words_freq[:20]\n",
    "\n",
    "# Separate words and counts\n",
    "words, counts = zip(*common_words)\n",
    "\n",
    "# Plot word frequencies\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(words, counts)\n",
    "plt.title('20 Most Common Words in Reviews')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Calculate the polarity\n",
    "def get_polarity(text):\n",
    "    textblob = TextBlob(str(text))\n",
    "    pol = textblob.sentiment.polarity # type: ignore\n",
    "    if(pol==0):\n",
    "        return \"Neutral\"\n",
    "    elif(pol>0.1 and pol<=1):\n",
    "        return \"Positive\"\n",
    "    elif(pol>-1 and pol<=-0.1):\n",
    "        return \"Negative\"\n",
    "    \n",
    "df_t1['polarity'] = df_t1['reviews_cleaned'].apply(get_polarity) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_t1['polarity'].value_counts())\n",
    "df_t1['polarity'].value_counts().plot(kind='pie', autopct='%1.0f%%', colors=[\"green\", \"yellow\", \"red\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "* Best results was achieved when removing stopwords before lemmatizing the reviews. If lemmatizing first lots of junk words like \"wa\" ended up in the top list due to the lemmatizer converting \"was\" to \"wa\".\n",
    "\n",
    "* Removal of contractions was also needed because otherwise words like isn't was split into \"is\" and \"n't\" and thus \"n't\" which isn't in the stop word dictionary wasn't cleaned."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task2: Text classification using Randomforest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read data to dataframe\n",
    "df_t2 = pd.read_csv('moviereviews.tsv', sep='\\t')\n",
    "\n",
    "#Print the number of null values\n",
    "print(df_t2.isnull().sum())\n",
    "\n",
    "# Drop NaN values\n",
    "df_t2.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "\n",
    "# Remove contractions function\n",
    "def remove_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "# Remove contractions\n",
    "df_t2['review'] = df_t2['review'].apply(remove_contractions)\n",
    "\n",
    "df_t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Remove special characters function\n",
    "def remove_special_characters(text):\n",
    "    return re.sub(r'[^A-Za-z0-9\\s]+', '', text)\n",
    "\n",
    "# Remove special characters\n",
    "df_t2['review'] = df_t2['review'].apply(remove_special_characters)\n",
    "\n",
    "df_t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokenize function\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# Tokenize\n",
    "df_t2['review_tokenized'] = df_t2['review'].apply(tokenize)\n",
    "\n",
    "df_t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase function\n",
    "def lowercase(tokens):\n",
    "    return [word.lower() for word in tokens]\n",
    "\n",
    "# Lowercase\n",
    "df_t2['review_tokenized'] = df_t2['review_tokenized'].apply(lowercase)\n",
    "\n",
    "df_t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Remove stopwords function\n",
    "def remove_stopwords(tokens):\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "# Remove stopwords\n",
    "df_t2['review_tokenized'] = df_t2['review_tokenized'].apply(remove_stopwords)\n",
    "\n",
    "df_t2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Create a WordNet lemmatizer object\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize function\n",
    "def lemmatize(tokens):\n",
    "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "# Lemmatize\n",
    "df_t2['review_tokenized'] = df_t2['review_tokenized'].apply(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Join the tokens back into strings\n",
    "df_t2['review_cleaned'] = df_t2['review_tokenized'].apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "# Create the CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the data\n",
    "X = vectorizer.fit_transform(df_t2['review_cleaned'])\n",
    "\n",
    "# Count the sum of each word\n",
    "sum_words = X.sum(axis=0)\n",
    "\n",
    "# Create a (word, frequency) list and sort it in descending order\n",
    "words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "\n",
    "# Get the 20 most common words\n",
    "common_words = words_freq[:20]\n",
    "\n",
    "# Separate words and counts\n",
    "words, counts = zip(*common_words)\n",
    "\n",
    "# Plot word frequencies\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(words, counts)\n",
    "plt.title('20 Most Common Words in Reviews')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing set (70 : 30)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_t2['review_cleaned'], df_t2['label'], test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "text_clf_forest = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Fit the model\n",
    "text_clf_forest.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test data\n",
    "y_pred = text_clf_forest.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy: {:.3f}'.format(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
