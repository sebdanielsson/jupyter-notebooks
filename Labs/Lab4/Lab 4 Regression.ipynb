{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GIK2FB Artificial Intelligence HT22\n",
    "\n",
    "## Lab 4: ML - Supervised Regression Learning, Naive Bayes (Probabilistic Classifier)\n",
    "\n",
    "Dalarna University\n",
    "(c) Joonas Pääkkönen, fall 2022\n",
    "\n",
    "### Deadline: Monday Dec 19th, 2022, 23:59 CET time\n",
    "\n",
    "Upload your .ipynb file to Learn. No video, just the .ipynb file. Simply write your code in the .ipynb file and submit it. Do not zip anything, like the datasets, with it.\n",
    "\n",
    "Use Scikit-learn first and foremost, with the help of Pandas, NumPy, SciPy, Matplotlib and Seaborn, if needed. Remember that Pandas dataframes come in very handy in data science.\n",
    "\n",
    "Note: plotting pairplots may take several seconds or even minutes.\n",
    "\n",
    "Make sure that your code runs before submitting.\n",
    "\n",
    "Remember to fill in the information below, i.e., student name(s) and email address(es).\n",
    "\n",
    "### Sebastian Danielsson: model solutions\n",
    "\n",
    "### h21sebda@du.se: model solutions\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1.1: Import Pandas, and download the hungary_chickenpox.csv dataset from\n",
    "https://archive.ics.uci.edu/ml/datasets/Hungarian+Chickenpox+Cases and save the data into a variable called \"chickendata\" with the read.csv() method of the Pandas module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Import data set and create a data frames\n",
    "chickendata = pd.read_csv('hungary_chickenpox.csv', delimiter=',', usecols=[*range(0, 21)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1.2: Print the chickendata dataset by simply writing the variable name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chickendata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1.3: Use the Pandas describe() method to describe chickendata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chickendata.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1.4: Use Seaborn to plot the pairplot of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.pairplot(chickendata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1.5: Use the scatterplot() method of Seaborn to plot a scatterplot where the x-coordinates are from the BUDAPEST column and the y-coordinates are from the VAS column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='BUDAPEST', y='VAS', data=chickendata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1.6: Use Scikit-learn to fit a simple linear OLS regression line to the data visualized in Task 1.5. Plot the linear regression line alongside the scatterplot of Task 1.5. Make sure the line and the scatterplot points are in different colors. Note: you may need to reshape the data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Input and output data\n",
    "x = chickendata['BUDAPEST'].values.reshape(-1,1)\n",
    "y = chickendata['VAS']\n",
    "\n",
    "# OLS linear regression model\n",
    "model_ols = LinearRegression()\n",
    "model_ols.fit(x, y)\n",
    "\n",
    "# Plot the regression line, small dots and black regression line\n",
    "sns.regplot(x='BUDAPEST', y='VAS', data=chickendata, scatter_kws={'s':2}, line_kws={\"color\": \"black\"})\n",
    "\n",
    "# Add more ticks to the x and y axis\n",
    "plt.xticks(range(0, 500+1, 50))\n",
    "plt.yticks(range(0, 150+1, 10))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1.7: What is the value of the coefficient of determination for the linear regression model of Task 1.6?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the R squared value\n",
    "r_squared = model_ols.score(x, y)\n",
    "\n",
    "# Print the R squared value\n",
    "print('R squared:', r_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1.8: Use Scikit-learn to fit an ordinary least squares linear regression model to predict the values of the BUDAPEST column with all the other data columns of the chickendata dataset, not only with the VAS column. Find the $R^2$-value for the prediction. Similarly, fit a Ridge regression model and also a LASSO regression model to the same data. Find the corresponding $R^2$-values for these two models as you did for the simple linear regression model. That is, you need to print all the three coefficients of determination to pass this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input and output data\n",
    "x = chickendata.drop(columns=['Date', 'BUDAPEST'])\n",
    "y = chickendata['BUDAPEST']\n",
    "\n",
    "# OLS linear regression model\n",
    "model_ols = LinearRegression()\n",
    "model_ols.fit(x, y)\n",
    "\n",
    "# R^2 value for the OLS model\n",
    "r_squared_ols = model_ols.score(x, y)\n",
    "\n",
    "# Ridge regression model\n",
    "model_ridge = Ridge()\n",
    "model_ridge.fit(x, y)\n",
    "\n",
    "# R^2 value for the Ridge model\n",
    "r_squared_ridge = model_ridge.score(x, y)\n",
    "\n",
    "# Lasso regression model\n",
    "model_lasso = Lasso()\n",
    "model_lasso.fit(x, y)\n",
    "\n",
    "# R^2 value for the Lasso model\n",
    "r_squared_lasso = model_lasso.score(x, y)\n",
    "\n",
    "# Print R^2 values\n",
    "print('R^2 (OLS):', r_squared_ols)\n",
    "print('R^2 (Ridge):', r_squared_ridge)\n",
    "print('R^2 (Lasso):', r_squared_lasso)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1.9: Now you have three different models and three different $R^2$-values. In terms of the $R^2$-value, which model performs the best? How would you comment these results in your own words? Enter your answer here:\n",
    "\n",
    "---\n",
    "The R squared values/scores from these three models are extremely similar and they were all pretty high scores. The OLS model had slightly higher score than the two other models and can be said to perform the best. Comparing the OLS model score when using all of the columns instead of only VAS increased to score tremendously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Binary classification with logistic regression and Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2.1: Download the DryBeanDataset.zip file from\n",
    "https://archive.ics.uci.edu/ml/datasets/Dry+Bean+Dataset and save Dry_Bean_Dataset.xlsx to variable \"beandata\". You need to read an .xlsx file now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Import data set and create a data frames\n",
    "beandata = pd.read_excel('Dry_Bean_Dataset.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2.2: Print the beandata dataset by simply writing the variable name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beandata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2.3: Use the describe() method to describe the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beandata.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2.4: What is the range (the difference between the maximum and the minimum) of the Area feature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(range(beandata['Area'].max(), beandata['Area'].min()))\n",
    "print('Area max value - min value:', (beandata['Area'].max() - beandata['Area'].min()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2.5: Use Seaborn to plot the pairplots of the dataset with the hue argument set to the class of the bean. Note that there are several kinds of beans. Follow the link in Task 2.1 if you want to read more details about the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot pairplots for Class\n",
    "sns.pairplot(beandata, hue='Class')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2.6: Imagine that you were to use a regression model to predict the Eccentricity feature as the regression target variable with the AspectRation feature as the independent variable. By looking at the corresponding pairplot above, what type of a regression model would you choose? Motivate your answer in your own words.\n",
    "\n",
    "Bonus: If you want, you can also apply the regression model of your choice here and plot the results, but this is not compulsory. Nonetheless, executing this bonus task can compensate for some shortcomings in the other tasks.\n",
    "\n",
    "---\n",
    "The pairplot of AspectRation and Eccentricity tells me that there is a strong linear relationship between the two variables. For the choice of a regression model the lasso model shouldn't be beneficial to us because we're only using one feature to predict our target variable. Let's try all of the three models down below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predict Eccentricity feature using AspectRation feature\n",
    "x = beandata['AspectRation'].values.reshape(-1,1)\n",
    "y = beandata['Eccentricity']\n",
    "\n",
    "# OLS linear regression model\n",
    "model_ols = LinearRegression()\n",
    "model_ols.fit(x, y)\n",
    "\n",
    "# Ridge regression model\n",
    "model_ridge = Ridge()\n",
    "model_ridge.fit(x, y)\n",
    "\n",
    "# Lasso regression model\n",
    "model_lasso = Lasso()\n",
    "model_lasso.fit(x, y)\n",
    "\n",
    "# Set up the figure and axes\n",
    "fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "# Plot OLS model\n",
    "ax[0].scatter(x, y, s=2)\n",
    "ax[0].plot(x, model_ols.predict(x), color='red')\n",
    "ax[0].set_title('OLS Regression')\n",
    "ax[0].set_xlabel('Aspect Ratio')\n",
    "ax[0].set_ylabel('Eccentricity')\n",
    "\n",
    "# Plot Ridge model\n",
    "ax[1].scatter(x, y, s=2)\n",
    "ax[1].plot(x, model_ridge.predict(x), color='green')\n",
    "ax[1].set_title('Ridge Regression')\n",
    "ax[1].set_xlabel('Aspect Ratio')\n",
    "ax[1].set_ylabel('Eccentricity')\n",
    "\n",
    "# Plot Lasso model\n",
    "ax[2].scatter(x, y, s=2)\n",
    "ax[2].plot(x, model_lasso.predict(x), color='blue')\n",
    "ax[2].set_title('Lasso Regression')\n",
    "ax[2].set_xlabel('Aspect Ratio')\n",
    "ax[2].set_ylabel('Eccentricity')\n",
    "\n",
    "# Get the R squared value\n",
    "r_squared_ols = model_ols.score(x, y)\n",
    "r_squared_ridge = model_ridge.score(x, y)\n",
    "r_squared_lasso = model_lasso.score(x, y)\n",
    "\n",
    "# Print the R squared value\n",
    "print('R squared:', r_squared_ols)\n",
    "print('R squared (Ridge):', r_squared_ridge)\n",
    "print('R squared (Lasso):', r_squared_lasso)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2.7: Let's say we are only interested in Bombay beans. Use Scikit-learn to build a logistic regression predictor based on the major axis length of the beans to separate Bombay beans from other beans. Plot the regression curve in the same figure with the corresponding scatterplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Get Major axis length (L) and Class columns\n",
    "X = beandata['MajorAxisLength'].values.reshape(-1, 1)\n",
    "y = beandata['Class'].values\n",
    "\n",
    "# Only select data that belongs to the BOMBAY class\n",
    "X_bombay = X[y == 'BOMBAY']\n",
    "y_bombay = y[y == 'BOMBAY']\n",
    "\n",
    "# Only select data that is not BOMBAY\n",
    "X_notbombay = X[y != 'BOMBAY']\n",
    "y_notbombay = y[y != 'BOMBAY']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Create the logistic regression model and fit it to the training data\n",
    "model_logistic = LogisticRegression(solver='lbfgs', max_iter=20000)\n",
    "model_logistic.fit(X_train, y_train)\n",
    "\n",
    "# Predict the class labels for the test data\n",
    "y_pred = model_logistic.predict(X_test)\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=[16,8])\n",
    "\n",
    "# Show subplot 1\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_bombay, y_bombay, s=2)\n",
    "plt.scatter(X_notbombay, y_notbombay, s=2, color='red')\n",
    "\n",
    "# Set Y axis tick marks to show probabilities from 0 to 1 in increments of 0.2\n",
    "plt.yticks(np.arange(0, 1.1, 0.2), ['0', '0.2', '0.4', '0.6', '0,8', '1'])\n",
    "\n",
    "# Generate values for BOMBAY\n",
    "x_range = np.arange(X_bombay.min(), X_bombay.max(), 0.1).reshape(-1, 1)\n",
    "\n",
    "# Make predictions\n",
    "y_probs = model_logistic.predict_proba(x_range)[:, 1]\n",
    "\n",
    "# Plot the regression curve\n",
    "plt.plot(x_range, y_probs, color='black')\n",
    "\n",
    "# Add labels\n",
    "plt.xlabel('Major Axis Length (L)')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Logistic Regression')\n",
    "\n",
    "# Show subplot 2\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_bombay, y_bombay, s=2)\n",
    "\n",
    "# Set Y axis tick marks to show probabilities from 0 to 1 in increments of 0.2\n",
    "plt.yticks(np.arange(0, 1.1, 0.05), ['0', '0.05', '0.1', '0.15', '0.2', '0.25', '0.3', '0.35', '0.4', '0.45', '0.5','0.55', '0.6', '0.65', '0.7', '0.75', '0.8', '0.85', '0.9', '0.95', '1', ''])\n",
    "\n",
    "# Generate values for BOMBAY\n",
    "x_range = np.arange(X_bombay.min(), X_bombay.max(), 0.1).reshape(-1, 1)\n",
    "\n",
    "# Make predictions\n",
    "y_probs = model_logistic.predict_proba(x_range)[:, 1]\n",
    "\n",
    "# Plot the regression curve\n",
    "plt.plot(x_range, y_probs, color='black')\n",
    "\n",
    "# Add labels\n",
    "plt.xlabel('Major Axis Length (L)')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Logistic Regression (BOMBAY)')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2.8: Use the score() method to evaluate the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Calculate the accuracy of the model's predictions for the \"BOMBAY\" class and all classes respectively\n",
    "accuracy_bombay = accuracy_score(y_test[y_test == 'BOMBAY'], y_pred[y_test == 'BOMBAY'])\n",
    "print('Accuracy for BOMBAY class:', accuracy_bombay)\n",
    "\n",
    "accuracy_overall = accuracy_score(y_test, y_pred)\n",
    "print('Overall accuracy:', accuracy_overall)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2.9: How would you describe the accuracy value? Write your answer in your own words here:\n",
    "\n",
    "---\n",
    "I think the accuracy is very good for classifying BOMBAY at roughly 0.9456 or 94.56%. I believe that this is due to the spread of values on MajorAxisLength feature where the BOMBAY beans are much larger than the rest if the classes and therefore making the classifications much easier to do. The overall accuracy for classifying all of the beans however was much worse at 0.6250 or about 62.5%. This is because the features of many types of the beans overlaps. The overall accuracy could of course be greatly improved by using more features to make the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2.10: Let's say we are still only interested in the Bombay beans of Task 2. Use Scikit-learn to build a Gaussian Naive Bayes (GaussianNB) predictor based on the major axis length of the beans to separate Bombay beans from other beans. Plot the regression curve in the same figure with the corresponding scatterplot. That is, repeat Task 2.7 but this time with the GaussianNB classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Get Major axis length (L) and Class columns\n",
    "X = beandata['MajorAxisLength'].values.reshape(-1, 1)\n",
    "y = beandata['Class'].values\n",
    "\n",
    "# Only select data that belongs to the BOMBAY class\n",
    "X_bombay = X[y == 'BOMBAY']\n",
    "y_bombay = y[y == 'BOMBAY']\n",
    "\n",
    "# Only select data that is not BOMBAY\n",
    "X_notbombay = X[y != 'BOMBAY']\n",
    "y_notbombay = y[y != 'BOMBAY']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Create the GaussianNB model and fit it to the training data\n",
    "model_gaussian = GaussianNB()\n",
    "model_gaussian.fit(X_train, y_train)\n",
    "\n",
    "# Predict the class labels for the test data\n",
    "y_pred = model_gaussian.predict(X_test)\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=[16,8])\n",
    "\n",
    "# Show subplot 1\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_bombay, y_bombay, s=2)\n",
    "plt.scatter(X_notbombay, y_notbombay, s=2, color='red')\n",
    "\n",
    "# Set Y axis tick marks to show probabilities from 0 to 1 in increments of 0.2\n",
    "plt.yticks(np.arange(0, 1.1, 0.2), ['0', '0.2', '0.4', '0.6', '0,8', '1'])\n",
    "\n",
    "# Generate values for BOMBAY\n",
    "x_range = np.arange(X_bombay.min(), X_bombay.max(), 0.1).reshape(-1, 1)\n",
    "\n",
    "# Make predictions\n",
    "y_probs = model_gaussian.predict_proba(x_range)[:, 1]\n",
    "\n",
    "# Plot the regression curve\n",
    "plt.plot(x_range, y_probs, color='black')\n",
    "\n",
    "# Add labels\n",
    "plt.xlabel('Major Axis Length (L)')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Gaussian Naive Bayes Regression')\n",
    "\n",
    "# Show subplot 2\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_bombay, y_bombay, s=2)\n",
    "\n",
    "# Set Y axis tick marks to show probabilities from 0 to 1 in increments of 0.2\n",
    "plt.yticks(np.arange(0, 1.1, 0.05), ['0', '0.05', '0.1', '0.15', '0.2', '0.25', '0.3', '0.35', '0.4', '0.45', '0.5','0.55', '0.6', '0.65', '0.7', '0.75', '0.8', '0.85', '0.9', '0.95', '1', ''])\n",
    "\n",
    "# Generate values for BOMBAY\n",
    "x_range = np.arange(X_bombay.min(), X_bombay.max(), 0.1).reshape(-1, 1)\n",
    "\n",
    "# Make predictions\n",
    "y_probs = model_gaussian.predict_proba(x_range)[:, 1]\n",
    "\n",
    "# Plot the regression curve\n",
    "plt.plot(x_range, y_probs, color='black')\n",
    "\n",
    "# Add labels\n",
    "plt.xlabel('Major Axis Length (L)')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Gaussian Naive Bayes Regression')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2.11: Use the score() method to evaluate the accuracy of the GaussianNB model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy of the model's predictions for the \"BOMBAY\" class and all classes respectively\n",
    "accuracy_bombay = accuracy_score(y_test[y_test == 'BOMBAY'], y_pred[y_test == 'BOMBAY'])\n",
    "print('Accuracy for BOMBAY class:', accuracy_bombay)\n",
    "\n",
    "accuracy_overall = accuracy_score(y_test, y_pred)\n",
    "print('Overall accuracy:', accuracy_overall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2.12: How many beans did the classifier misclassify?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# Confusion matrix for the gaussian model\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred, display_labels=['BARB.', 'BOMBAY', 'CALI', 'DERM.', 'HOROZ','SEKER', 'SIRA'])\n",
    "\n",
    "print('The model missclassified four of the BOMBAY beans as CALI. No beans were missclassified as BOMBAY.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2.13: How would you comment the results in your own words? Compare the logistic regression model and the Gaussian Naive Bayes model performance. Write your answer in your own words here:\n",
    "\n",
    "---\n",
    "The results for both the logistic regression model and the Gaussian Naive Bayes model performance are extremely similar and both got a very high accuracy score. As explained earlier I still believe this to be due to the MajorAxisLength feature for BOMBAY being noticably higher than for the rest of the beans. Any the pretty poor overall accuracy to be due to the features for many of the other beans to be similar. Adding one or more features should improve the accuracy.\n",
    "\n",
    "Gaussian NB were slightly more accurate but with these small differences it's almost not worth pointing out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GIK2FB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Nov 24 2022, 08:08:27) [Clang 14.0.6 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "73a4deffa37449eec2906a82541729956dc4ede81d351440dc1e5a408dd292eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
